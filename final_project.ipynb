{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "1. Formatting the csv file\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uHb9uFORwQRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the semicolon-separated data\n",
        "df = pd.read_csv('labelled_newscatcher_dataset.csv', sep=';')\n",
        "\n",
        "# Rename the columns\n",
        "df.columns = ['topic', 'link', 'domain', 'published_date', 'title', 'lang']\n",
        "\n",
        "# Save to a new CSV\n",
        "df.to_csv('cleaned_data.csv', index=False)"
      ],
      "metadata": {
        "id": "9aoVWkskD6Iw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "2. Import libraries\n",
        "```"
      ],
      "metadata": {
        "id": "-XFhCsizSAvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, regexp_replace, trim, lower\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        ")\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time"
      ],
      "metadata": {
        "id": "c-4sdthMP_cz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "3. Text Processing and Cleaning\n",
        "```"
      ],
      "metadata": {
        "id": "4k8VAsEXSJDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NewsClassificationModels\").getOrCreate()\n",
        "\n",
        "# 2. Load data\n",
        "df = spark.read.csv(\"cleaned_data.csv\", header=True, inferSchema=True).fillna({\"title\": \"\"})\n",
        "\n",
        "# 3. Clean the text\n",
        "df_clean = df.withColumn(\"title\", regexp_replace(col(\"title\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        "df_clean = df_clean.withColumn(\"title\", regexp_replace(col(\"title\"), r\"\\s+\", \" \"))\n",
        "df_clean = df_clean.withColumn(\"title\", trim(col(\"title\")))\n",
        "df_clean = df_clean.withColumn(\"title\", lower(col(\"title\")))"
      ],
      "metadata": {
        "id": "eUFHhc7iP0Lc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing sample rows\n",
        "print(\"Sample rows:\")\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQWd-SCShKOs",
        "outputId": "5a9955c9-f7bf-45b1-c433-c7e713a21506"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rows:\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+----+\n",
            "|topic  |link                                                                                                                       |domain        |published_date     |title                                                                                               |lang|\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+----+\n",
            "|SCIENCE|https://www.eurekalert.org/pub_releases/2020-08/dbnl-acl080620.php                                                         |eurekalert.org|2020-08-06 13:59:45|A closer look at water-splitting's solar fuel potential                                             |en  |\n",
            "|SCIENCE|https://www.pulse.ng/news/world/an-irresistible-scent-makes-locusts-swarm-study-finds/jy784jw                              |pulse.ng      |2020-08-12 15:14:19|An irresistible scent makes locusts swarm, study finds                                              |en  |\n",
            "|SCIENCE|https://www.express.co.uk/news/science/1322607/artificial-intelligence-warning-machine-learning-algorithm-social-media-data|express.co.uk |2020-08-13 21:01:00|Artificial intelligence warning: AI will know us better than we know ourselves                      |en  |\n",
            "|SCIENCE|https://www.ndtv.com/world-news/glaciers-could-have-sculpted-mars-valleys-study-2273648                                    |ndtv.com      |2020-08-03 22:18:26|Glaciers Could Have Sculpted Mars Valleys: Study                                                    |en  |\n",
            "|SCIENCE|https://www.thesun.ie/tech/5742187/perseid-meteor-shower-tonight-time-uk-see/                                              |thesun.ie     |2020-08-12 19:54:36|Perseid meteor shower 2020: What time and how to see the huge bright FIREBALLS over UK again tonight|en  |\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing row count\n",
        "print(f\"Total rows: {df.count()}\")\n",
        "\n",
        "# Assuming 'Category' is the label column - printing distinct categories and their counts\n",
        "print(\"Category counts:\")\n",
        "df.groupBy(\"topic\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MG4aHYqYAaF",
        "outputId": "2b1d67f3-0fb8-4a7b-8ebd-a5d89dcf76c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 78731\n",
            "Category counts:\n",
            "+-------------+-----+\n",
            "|        topic|count|\n",
            "+-------------+-----+\n",
            "|       SPORTS|10135|\n",
            "|ENTERTAINMENT|10680|\n",
            "|     BUSINESS| 9673|\n",
            "|       HEALTH|11853|\n",
            "|        WORLD|11616|\n",
            "|   TECHNOLOGY|12103|\n",
            "|       NATION| 9681|\n",
            "|      SCIENCE| 2990|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "4. Checking Different Models\n",
        "```"
      ],
      "metadata": {
        "id": "P7BlqBAwyOXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier, LinearSVC, OneVsRest\n",
        "import time\n",
        "\n",
        "# 4. Preprocessing stages\n",
        "label_indexer = StringIndexer(inputCol=\"topic\", outputCol=\"label\")\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words_token\")\n",
        "remover = StopWordsRemover(inputCol=\"words_token\", outputCol=\"words_clean\")\n",
        "hashingTF = HashingTF(inputCol=\"words_clean\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# 5. Split data\n",
        "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 6. Define evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# 7. Logistic Regression pipeline and evaluation\n",
        "print(\"\\nğŸ” Training Logistic Regression...\")\n",
        "start = time.time()\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lr_pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    hashingTF,\n",
        "    idf,\n",
        "    lr\n",
        "])\n",
        "\n",
        "lr_model = lr_pipeline.fit(train_data)\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "elapsed_lr = time.time() - start\n",
        "\n",
        "print(f\"âœ… Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"â±ï¸ Time taken: {elapsed_lr:.2f} seconds\")\n",
        "\n",
        "# 8. Naive Bayes pipeline and evaluation\n",
        "print(\"\\nğŸ” Training Naive Bayes...\")\n",
        "start = time.time()\n",
        "\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", smoothing=1.0, modelType=\"multinomial\")\n",
        "nb_pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    hashingTF,\n",
        "    idf,\n",
        "    nb\n",
        "])\n",
        "\n",
        "nb_model = nb_pipeline.fit(train_data)\n",
        "nb_predictions = nb_model.transform(test_data)\n",
        "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
        "elapsed_nb = time.time() - start\n",
        "\n",
        "print(f\"âœ… Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
        "print(f\"â±ï¸ Time taken: {elapsed_nb:.2f} seconds\")\n",
        "\n",
        "# 9. Random Forest pipeline and evaluation\n",
        "print(\"\\nğŸŒ² Training Random Forest Classifier...\")\n",
        "start = time.time()\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
        "rf_pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    hashingTF,\n",
        "    idf,\n",
        "    rf\n",
        "])\n",
        "\n",
        "rf_model = rf_pipeline.fit(train_data)\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
        "elapsed_rf = time.time() - start\n",
        "\n",
        "print(f\"âœ… Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"â±ï¸ Time taken: {elapsed_rf:.2f} seconds\")\n",
        "\n",
        "# 10. Linear SVC pipeline and evaluation\n",
        "print(\"\\nâš¡ Training Linear SVC (One-vs-Rest)...\")\n",
        "start = time.time()\n",
        "\n",
        "svc = LinearSVC(maxIter=100, regParam=0.1)\n",
        "ovr = OneVsRest(classifier=svc, featuresCol=\"features\", labelCol=\"label\")\n",
        "svc_pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    hashingTF,\n",
        "    idf,\n",
        "    ovr\n",
        "])\n",
        "\n",
        "svc_model = svc_pipeline.fit(train_data)\n",
        "svc_predictions = svc_model.transform(test_data)\n",
        "svc_accuracy = evaluator.evaluate(svc_predictions)\n",
        "elapsed_svc = time.time() - start\n",
        "\n",
        "print(f\"âœ… Linear SVC Accuracy: {svc_accuracy:.4f}\")\n",
        "print(f\"â±ï¸ Time taken: {elapsed_svc:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF2SUC9T8U9U",
        "outputId": "f1c4a5a7-31a2-4b4c-acdc-c4accfa4ab29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Training Logistic Regression...\n",
            "âœ… Logistic Regression Accuracy: 0.7379\n",
            "â±ï¸ Time taken: 51.47 seconds\n",
            "\n",
            "ğŸ” Training Naive Bayes...\n",
            "âœ… Naive Bayes Accuracy: 0.7293\n",
            "â±ï¸ Time taken: 17.32 seconds\n",
            "\n",
            "ğŸŒ² Training Random Forest Classifier...\n",
            "âœ… Random Forest Accuracy: 0.3879\n",
            "â±ï¸ Time taken: 116.19 seconds\n",
            "\n",
            "âš¡ Training Linear SVC (One-vs-Rest)...\n",
            "âœ… Linear SVC Accuracy: 0.7371\n",
            "â±ï¸ Time taken: 220.64 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "5. Logistic Regression Model Building and Training\n",
        "```"
      ],
      "metadata": {
        "id": "jLiJGX-TB-nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace, trim, lower, col\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF,\n",
        "    StringIndexer, IndexToString\n",
        ")\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time\n",
        "\n",
        "# 1. Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"NewsClassification_LogisticRegression\").getOrCreate()\n",
        "\n",
        "# 2. Load and clean data\n",
        "df = spark.read.csv(\"cleaned_data.csv\", header=True, inferSchema=True).fillna({\"title\": \"\"})\n",
        "df_clean = df.withColumn(\"title\", regexp_replace(col(\"title\"), r\"[^a-zA-Z\\s]\", \" \"))\n",
        "df_clean = df_clean.withColumn(\"title\", regexp_replace(col(\"title\"), r\"\\s+\", \" \"))\n",
        "df_clean = df_clean.withColumn(\"title\", trim(lower(col(\"title\"))))\n",
        "\n",
        "# 3. Define pipeline stages\n",
        "label_indexer = StringIndexer(inputCol=\"topic\", outputCol=\"label\")\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words_token\")\n",
        "remover = StopWordsRemover(inputCol=\"words_token\", outputCol=\"words_clean\")\n",
        "\n",
        "# Increase HashingTF numFeatures to 50000\n",
        "hashingTF = HashingTF(inputCol=\"words_clean\", outputCol=\"raw_features\", numFeatures=100000)\n",
        "\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"tfidf_features\", labelCol=\"label\", maxIter=20, regParam=0.3, elasticNetParam=0.0)\n",
        "\n",
        "# Convert prediction index back to topic label\n",
        "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_topic\", labels=label_indexer.fit(df_clean).labels)\n",
        "\n",
        "# 4. Train-test split\n",
        "train_data, val_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 5. Build the pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    tokenizer,\n",
        "    remover,\n",
        "    hashingTF,\n",
        "    idf,\n",
        "    lr,\n",
        "    label_converter\n",
        "])\n",
        "\n",
        "# 6. Train and evaluate\n",
        "print(\"\\nğŸ” Training model: Logistic Regression + HashingTF\")\n",
        "start = time.time()\n",
        "\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(val_data)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
        "print(f\"â±ï¸ Time taken: {elapsed:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3wTJmUYt-y-",
        "outputId": "761feebf-b3a8-4427-f9e7-10121dc3aa08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Training model: Logistic Regression + HashingTF\n",
            "âœ… Accuracy: 0.7964\n",
            "â±ï¸ Time taken: 61.98 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "6. Model Prediction with Custom News Headline\n",
        "```"
      ],
      "metadata": {
        "id": "bC1-edctSydo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "new_text = \"5 symptoms of colon cancer that should not be ignored\"\n",
        "new_df = spark.createDataFrame([Row(Text=new_text)])\n",
        "\n",
        "# 2. Clean the text (same steps as training data)\n",
        "new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        "new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"\\s+\", \" \"))\n",
        "\n",
        "# 3. Transform using the trained model pipeline\n",
        "prediction_result = model.transform(new_df)\n",
        "\n",
        "# 4. Show prediction\n",
        "prediction_result.select(\"Text\", \"prediction\").show()\n",
        "\n",
        "# Get the original labels from the label indexer (it's the first stage in the pipeline)\n",
        "labels = model.stages[0].labels\n",
        "\n",
        "# Get the prediction value\n",
        "predicted_index = prediction_result.select(\"prediction\").first()[\"prediction\"]\n",
        "\n",
        "# Map index to actual category\n",
        "predicted_category = labels[int(predicted_index)]\n",
        "\n",
        "print(f\"Predicted category: {predicted_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snbd1vA8QgeI",
        "outputId": "120ebe52-1482-46d0-b594-cfbb9b7d2053"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|                Text|prediction|\n",
            "+--------------------+----------+\n",
            "|5 symptoms of col...|       1.0|\n",
            "+--------------------+----------+\n",
            "\n",
            "Predicted category: HEALTH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "new_text = \"The championship final was an intense battle as the underdog team clinched the title in the last minute of the game. Fans celebrated wildly after a stunning comeback that showcased exceptional teamwork and determination. Experts say this victory could redefine the teamâ€™s future in the league.\"\n",
        "new_df = spark.createDataFrame([Row(Text=new_text)])\n",
        "\n",
        "# 2. Clean the text (same steps as training data)\n",
        "new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        "new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"\\s+\", \" \"))\n",
        "\n",
        "# 3. Transform using the trained model pipeline\n",
        "prediction_result = model.transform(new_df)\n",
        "\n",
        "# 4. Show prediction\n",
        "prediction_result.select(\"Text\", \"prediction\").show()\n",
        "\n",
        "# Get the original labels from the label indexer (it's the first stage in the pipeline)\n",
        "labels = model.stages[0].labels\n",
        "\n",
        "# Get the prediction value\n",
        "predicted_index = prediction_result.select(\"prediction\").first()[\"prediction\"]\n",
        "\n",
        "# Map index to actual category\n",
        "predicted_category = labels[int(predicted_index)]\n",
        "\n",
        "print(f\"Predicted category: {predicted_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unzcwVLAU4zD",
        "outputId": "51dc6fc8-3a3e-441b-b213-c598716ab61e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|                Text|prediction|\n",
            "+--------------------+----------+\n",
            "|The championship ...|       4.0|\n",
            "+--------------------+----------+\n",
            "\n",
            "Predicted category: SPORTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "7. Gradio Libraries Installation\n",
        "```"
      ],
      "metadata": {
        "id": "xxZLGXunBoRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gradio --quiet\n",
        "!pip install deep-translator --quiet\n",
        "\n",
        "# Imports\n",
        "import gradio as gr\n",
        "from pyspark.sql.functions import regexp_replace, trim, lower, col\n",
        "from pyspark.sql import Row\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Translation function\n",
        "def translate_to_english(text: str) -> str:\n",
        "    try:\n",
        "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
        "    except Exception as e:\n",
        "        print(\"Translation failed:\", e)\n",
        "        return text\n",
        "\n",
        "# News category prediction function\n",
        "def predict_category(headline: str):\n",
        "    translated = translate_to_english(headline)\n",
        "\n",
        "    new_df = spark.createDataFrame([Row(Text=translated)])\n",
        "    new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"[^a-zA-Z\\s]\", \" \"))\n",
        "    new_df = new_df.withColumn(\"title\", regexp_replace(col(\"Text\"), r\"\\s+\", \" \"))\n",
        "    new_df = new_df.withColumn(\"title\", trim(lower(col(\"Text\"))))\n",
        "\n",
        "    prediction_result = model.transform(new_df)\n",
        "    labels = model.stages[0].labels\n",
        "    predicted_index = prediction_result.select(\"prediction\").first()[\"prediction\"]\n",
        "    predicted_category = labels[int(predicted_index)]\n",
        "\n",
        "    return translated, predicted_category\n",
        "\n",
        "\n",
        "# Clear input/output\n",
        "def clear_fields():\n",
        "    return \"\", \"\", \"\""
      ],
      "metadata": {
        "id": "pqhKdfVW3rhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3080ed3e-0df0-44b9-ae87-b986d8c3bcaf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "8. Web Interface Designing\n",
        "```"
      ],
      "metadata": {
        "id": "hWzTShurAMoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(css=\"\"\"\n",
        "    #root {\n",
        "        background-color: #e0e0e0 !important;\n",
        "        display: flex;\n",
        "        justify-content: center;\n",
        "        align-items: flex-start;\n",
        "        height: 100vh;\n",
        "        flex-direction: column;\n",
        "        padding-top: 100px;\n",
        "    }\n",
        "\n",
        "    #center-box {\n",
        "        max-width: 700px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    .wide-textbox textarea {\n",
        "        min-height: 100px !important;\n",
        "        font-size: 24px;\n",
        "    }\n",
        "\n",
        "    .center-bold-text textarea,\n",
        "    .center-bold-text input {\n",
        "        text-align: center !important;\n",
        "        font-weight: bold !important;\n",
        "        font-size: 20px;\n",
        "    }\n",
        "\n",
        "    .large-font textarea {\n",
        "        font-size: 24px !important;\n",
        "    }\n",
        "\"\"\", theme=\"soft\") as demo:\n",
        "\n",
        "\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    <div style='text-align: center'>\n",
        "        <h1 style='font-size: 24px; font-weight: bold;'>ğŸ§  Multilingual News Topic Classifier</h1>\n",
        "        <p style='font-size: 20px;'>Enter a news headline in any language and get its translated version and predicted topic.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Column(elem_id=\"center-box\"):\n",
        "        # Input\n",
        "        headline_input = gr.Textbox(\n",
        "            lines=2,\n",
        "            max_lines=5,\n",
        "            placeholder=\"Type a news headline in any language...\",\n",
        "            label=\"ğŸ“° News Headline\",\n",
        "            elem_classes=[\"spaced\", \"wide-textbox\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            submit_button = gr.Button(\"ğŸ” Predict\", elem_classes=[\"spaced\"])\n",
        "            clear_button = gr.Button(\"ğŸ§¹ Clear\", elem_classes=[\"spaced\"])\n",
        "\n",
        "        # Output boxes\n",
        "        translated_output = gr.Textbox(\n",
        "            label=\"ğŸ—£ï¸ Translated Headline\",\n",
        "            lines=2,\n",
        "            interactive=False,\n",
        "             elem_classes=[\"spaced\",\"large-font\"]\n",
        "        )\n",
        "\n",
        "        category_output = gr.Textbox(\n",
        "            label=\"ğŸ“Œ Predicted Category\",\n",
        "            lines=1,\n",
        "            interactive=False,\n",
        "            elem_classes=[\"center-bold-text\"]\n",
        "        )\n",
        "\n",
        "    # Actions\n",
        "    submit_button.click(fn=predict_category, inputs=headline_input, outputs=[translated_output, category_output])\n",
        "    clear_button.click(fn=clear_fields, inputs=[], outputs=[headline_input, translated_output, category_output])\n",
        "\n",
        "demo.launch(share=True, inline=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhZ8VY91ACaE",
        "outputId": "7122256d-ef01-42f4-a1f1-c72e7a6bda83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://05a77222a757f12da7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}